{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, training_data, field_types):\n",
    "        self.training_data = np.array(training_data)\n",
    "        self.field_types = field_types\n",
    "\n",
    "    def mean_squared_error(self, records):\n",
    "        if records.shape[0] == 0:\n",
    "            return 0\n",
    "        targets = records[:, -1].astype('float')\n",
    "        value = np.mean(targets)\n",
    "        mse = np.mean(np.square(targets - value))\n",
    "        return mse\n",
    "\n",
    "    def find_best_attr(self, records):\n",
    "        result = {'attr': None, 'split_cond': None, 'splits': None}\n",
    "        min_mse = -1\n",
    "        for i in np.arange(0, records.shape[1] - 1):\n",
    "            split_cond = None\n",
    "            splits = {}\n",
    "            if self.field_types[i] == 'N':\n",
    "                # numerical attribute\n",
    "                left_split = list()\n",
    "                right_split = list()\n",
    "                split_cond = np.mean(records[:, i].astype('float')) # mean value of attribute i\n",
    "                for record in records:\n",
    "                    if float(record[i]) < split_cond:\n",
    "                        left_split.append(record)\n",
    "                    else:\n",
    "                        right_split.append(record)\n",
    "                splits['left'] = np.array(left_split)\n",
    "                splits['right'] = np.array(right_split)\n",
    "            else:\n",
    "                # categorical attribute\n",
    "                split_cond = list(set(records[:, i]))\n",
    "                splits = {cond:list() for cond in split_cond}\n",
    "                for record in records:\n",
    "                    splits[record[i]].append(record)\n",
    "                splits = {k: np.array(v) for k,v in splits.items()}\n",
    "\n",
    "            # calculate MSE of splits\n",
    "            error = 0\n",
    "            for cond in splits:\n",
    "                split = splits[cond]\n",
    "                error += (split.shape[0]/records.shape[0])*self.mean_squared_error(split)\n",
    "            \n",
    "            if min_mse == -1 or error < min_mse:\n",
    "                result['attr'] = i # index of chosen attribute for splitting\n",
    "                result['split_cond'] = split_cond\n",
    "                result['splits'] = splits\n",
    "                min_mse = error\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def split(self, node):\n",
    "        # split the records in a node, apply recursively to child nodes\n",
    "        splits = node['splits']\n",
    "        min_record = 5 # if the number of record in a split < min_record, we make a leaf node\n",
    "        for i in splits:\n",
    "            split = splits[i]\n",
    "            if split.shape[0] <= min_record:\n",
    "                node[i] = np.mean(split[:, -1].astype('float')) # make a leaf node\n",
    "            else:\n",
    "                node[i] = self.find_best_attr(split) # make an internal node\n",
    "                self.split(node[i]) # split recursively on the child node\n",
    "\n",
    "    def build_model(self):\n",
    "        root_node = self.find_best_attr(self.training_data)\n",
    "        self.split(root_node)\n",
    "        return root_node\n",
    "        \n",
    "    def apply_model(self, node, record):\n",
    "        if self.field_types[node['attr']] == 'N':\n",
    "            # numerical node\n",
    "            if float(record[node['attr']]) < node['split_cond']:\n",
    "                if isinstance(node['left'], dict):\n",
    "                    return self.apply_model(node['left'], record)\n",
    "                else:\n",
    "                    return node['left'] # leaf node\n",
    "            else:\n",
    "                if isinstance(node['right'], dict):\n",
    "                    return self.apply_model(node['right'], record)\n",
    "                else:\n",
    "                    return node['right'] # leaf node\n",
    "        else:\n",
    "            # categorical node\n",
    "            cat = record[node['attr']]\n",
    "            if cat not in node['split_cond'] and len(node['split_cond']) > 0:\n",
    "                # not equal to any categorical values, set to the first category as default\n",
    "                cat = node['split_cond'][0]\n",
    "            \n",
    "            if isinstance(node[cat], dict):\n",
    "                return self.apply_model(node[cat], record)\n",
    "            else:\n",
    "                return node[cat] # leaf node\n",
    "\n",
    "    def predict(self, model, test_data):\n",
    "        predictions = []\n",
    "        for record in test_data:\n",
    "            pred_val = self.apply_model(model, record)\n",
    "            predictions.append([float(record[-1]), pred_val])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on = 'departure'\n",
    "maxSplitNumber = 1000\n",
    "sampleSize = 4000\n",
    "sampleNumber = 3\n",
    "field_types = ['C', 'N', 'C', 'C', 'C', 'C', 'C', 'C', 'N', 'N', 'N', 'N', 'N', 'N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_rdd = sc.textFile(\"/mnt/wiktorskit-jungwonseo-ns0000k/home/notebook/group03/Predict-Delay/Dataset/cleaned_train_whole.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rdd = sc.textFile(\"/mnt/wiktorskit-jungwonseo-ns0000k/home/notebook/group03/Predict-Delay/Dataset/cleaned_test_2000.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNonPredict(line, predict_on):\n",
    "    if predict_on == 'departure':\n",
    "        del line[-1]\n",
    "    else:\n",
    "        del line[-2]\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceSplit(x, y):\n",
    "    x.extend(y)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samplingData(key, values):\n",
    "    values_list = np.array(values)\n",
    "    l = values_list.shape[0]\n",
    "    if l < sampleSize:\n",
    "        return [(key, values)]\n",
    "    else:\n",
    "        # sampling without replacement\n",
    "        pairs = []\n",
    "        for i in range(0, sampleNumber):\n",
    "            idx = np.random.choice(l, size=sampleSize, replace=False)\n",
    "            pairs.append((\"{}_{}\".format(key, i), values_list[idx, :].tolist()))\n",
    "        return pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(training_set, test_set):\n",
    "    DT = DecisionTree(training_set, field_types)\n",
    "    model = DT.build_model()\n",
    "    predictions = DT.predict(model, test_set)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = np.array(test_rdd\n",
    "                    .map(lambda line: line.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").split(','))\n",
    "                    .map(lambda line: removeNonPredict(line, predict_on))\n",
    "                    .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time start 1555268264.282026\n",
      "Elapsed time: 111.44248723983765\n",
      "time start 1555268375.7246552\n",
      "Elapsed time: 110.4754331111908\n",
      "time start 1555268486.200298\n",
      "Elapsed time: 106.6352527141571\n"
     ]
    }
   ],
   "source": [
    "elapsed_time = []\n",
    "for i in range(0,3):\n",
    "    time_start=time.time()\n",
    "    print(\"time start\",time_start)\n",
    "\n",
    "    result = (training_rdd\n",
    "            .map(lambda line: line.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").split(','))\n",
    "            .map(lambda line: (random.randint(0, maxSplitNumber), [removeNonPredict(line, predict_on)]))\n",
    "            .reduceByKey(reduceSplit)\n",
    "            .flatMap(lambda record: samplingData(record[0], record[1]))\n",
    "            .map(lambda training_set: (1, predict(training_set[1], test_set)))\n",
    "            .reduce(lambda x, y: (x[0] + y[0], np.add(x[1], y[1]))))\n",
    "\n",
    "    time_end=time.time()\n",
    "    print(\"Elapsed time:\", time_end - time_start)\n",
    "    elapsed_time.append(time_end - time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109.51772435506184\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.        , 10.03111333],\n",
       "       [ 5.        ,  8.25696526],\n",
       "       [22.        ,  6.34568765],\n",
       "       ...,\n",
       "       [-6.        ,  8.04308469],\n",
       "       [-2.        ,  8.66230991],\n",
       "       [62.        , 10.63738484]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions = result[1]/result[0]\n",
    "final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.051454747262607\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(np.mean(np.sum(np.square(final_predictions),axis=1)))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
