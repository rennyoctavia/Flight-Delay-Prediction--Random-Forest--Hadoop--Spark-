{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.linalg import SparseVector, Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import StringIndexer,VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "import time\n",
    "import sys\n",
    "from operator import add\n",
    "import pyspark\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lines,data_group,predict):\n",
    "    line = lines.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").split(',')\n",
    "    \n",
    "    if predict == 'departure':\n",
    "        target = float(line[12])\n",
    "    else:\n",
    "        target = float(line[13])\n",
    "        \n",
    "    # Month (idx: 1) categorical (idx 0)\n",
    "    # Day_of_month (idx:2) numerical (idx 1)\n",
    "    # day_of_week, (idx:3) categorical (idx 2)\n",
    "    # Op_unique_carrier,(4) categorical (idx 3)\n",
    "    # Tail_num (5) categorical (idx 4)\n",
    "    # Op_carrier_fl_num (6) categorical (idx 5)\n",
    "    # Origin (7) categorical (idx 6)\n",
    "    # Dest (8) categorical (idx 7)\n",
    "    # Crs_dep_time (9) numerical (idx 8)\n",
    "    # Crs_arr_time (14) numerical (idx 9)\n",
    "    # Csr_elapsed_time, (20) numerical (idx 10)\n",
    "    # Distance (23) numerical (idx 11)\n",
    "    # dep_delay(target) (11) numerical (idx 12)\n",
    "    # arr_delay(target) (16) numerical (idx 13)\n",
    "        \n",
    "    return ((target,line[0],int(line[1]),line[2],line[3],line[4],line[5],line[6],line[7],int(line[8]),int(line[9]),float(line[10]),float(line[11]),data_group))    \n",
    "  \n",
    "\n",
    "def get_train_test(file_path,library,predict):\n",
    "    \n",
    "    # Reading train and test data and convert them\n",
    "    raw_train = sc.textFile(\"file://\"+file_path[0])\n",
    "    train_conv = raw_train.map(lambda x : convert(x,'train',predict))\n",
    "    \n",
    "    raw_test = sc.textFile(\"file://\"+file_path[1])\n",
    "    test_conv = raw_test.map(lambda x : convert(x,'test',predict))\n",
    "\n",
    "    # Merge the train and test data before converting categorical_features into numerical category and create dataframe\n",
    "    raw_all = sc.union([train_conv, test_conv])\n",
    "    df = sqlContext.createDataFrame(raw_all, schema = ['label', '0','1','2','3','4','5','6','7','8','9','10','11','data_group'])\n",
    "    \n",
    "    # Define which columns are numerical and which are categorical\n",
    "    numerical_features = ['1','8','9','10','11']\n",
    "    categorical_features = ['0', '2', '3', '4', '5', '6', '7']\n",
    "    \n",
    "    \n",
    "    stages = [] # to save all stages for the Pipeline\n",
    "    \n",
    "    # Convert categorical features into numerical category using stringIndexer and save the stages for pipeline\n",
    "    for categoricalCol in categorical_features:\n",
    "        stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Indexer\")\n",
    "        stages += [stringIndexer]\n",
    "    \n",
    "    # will group all the features into one vector : categorical_features(which have been converted to number) + numerical_features\n",
    "    assemblerInputs = [x + \"Indexer\" for x in categorical_features] + numerical_features\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    stages += [assembler]\n",
    "    \n",
    "    # Create a Pipeline, and transform the dataframe with above changes listed in array stages\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    pipelineModel = pipeline.fit(df)\n",
    "    df_model = pipelineModel.transform(df)\n",
    "    \n",
    "    # split back into train and test dataframes by filtering train and test\n",
    "    df_train = df_model.filter(\"data_group=='train'\").select('label','features') \n",
    "    df_test = df_model.filter(\"data_group=='test'\").select('label','features')\n",
    "    \n",
    "    #if using mllib library, convert dataframe to RDD LabeledPoint for train and test data\n",
    "    if library == 'mllib':\n",
    "        data_LP_train = df_train.rdd.map(lambda x: LabeledPoint(x[0], x[1].toArray()))\n",
    "        data_LP_test = df_test.rdd.map(lambda x: LabeledPoint(x[0], x[1].toArray()))\n",
    "        return data_LP_train,data_LP_test\n",
    "    \n",
    "    #if using ml library\n",
    "    else:\n",
    "        return df_model2\n",
    "    #\n",
    "\n",
    "def mllib_random_forest(max_depth,num_trees,train_set):\n",
    "    #Random Forest Regressor implementation\n",
    "    model_rf = RandomForest.trainRegressor(train_set, categoricalFeaturesInfo={},\n",
    "                                        numTrees=num_trees, featureSubsetStrategy=\"auto\",\n",
    "                                        impurity='variance', maxDepth=max_depth)\n",
    " \n",
    "    return model_rf\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # default on what to predict, can be set to 'arrival'\n",
    "    predict = 'departure'\n",
    "    library = 'mllib'\n",
    "    \n",
    "    #training and test input data path, index 0 = train, index 1 = test\n",
    "    input_path = [\"/mnt/wiktorskit-jungwonseo-ns0000k/home/notebook/group03/Predict-Delay/Dataset/cleaned_train_whole.txt\",\\\n",
    "                  \"/mnt/wiktorskit-jungwonseo-ns0000k/home/notebook/group03/Predict-Delay/Dataset/cleaned_test_2000.txt\"]\n",
    "    output_path = \"/mnt/wiktorskit-jungwonseo-ns0000k/home/notebook/group03/Renny-temp/predictions01\"\n",
    "    \n",
    "    if not os.path.isfile(input_path[0]) or not os.path.isfile(input_path[1]) or \"/mnt/\" not in input_path[0] or \"/mnt/\" not in input_path[1] or len(output_path)==0:\n",
    "        print(\"Please check your input path again\")\n",
    "        sys.exit(-1)\n",
    "\n",
    "    sc = pyspark.SparkContext.getOrCreate()\n",
    "    sqlContext = SQLContext(sc)\n",
    "    \n",
    "    print('converting train and test data ..')\n",
    "    train,test = get_train_test(input_path,library,predict)\n",
    "    print('converting data done')\n",
    "    \n",
    "    runtime = []\n",
    "    rmse = []\n",
    "    \n",
    "    for i in range(3):   \n",
    "        if library == 'mllib':\n",
    "            print('Building random forest model ..')\n",
    "            time_start=time.time()\n",
    "            model_rf = mllib_random_forest(5,1000,train)\n",
    "\n",
    "\n",
    "        print('Run prediction to test data')\n",
    "        predictions = model_rf.predict(test.map(lambda x: x.features))\n",
    "        labelsAndPredictions = test.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "        testMSE = labelsAndPredictions.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() /\\\n",
    "            float(test.count())\n",
    "\n",
    "        time_end=time.time()\n",
    "        time_rf=(time_end - time_start)\n",
    "        print(\"Build Random Forest and do predictions takes %d s\" %(time_rf))\n",
    "        \n",
    "        runtime.append(time_rf)\n",
    "\n",
    "        print('Test Mean Squared Error = ' + str(testMSE**0.5))\n",
    "\n",
    "        # first metrics\n",
    "        from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "        # Instantiate metrics object\n",
    "        metrics = RegressionMetrics(labelsAndPredictions)\n",
    "\n",
    "        # Squared Error\n",
    "        print(\"MSE = %s\" % metrics.meanSquaredError)\n",
    "        rmse.append(metrics.rootMeanSquaredError)\n",
    "        print(\"RMSE = %s\" % metrics.rootMeanSquaredError)\n",
    "        \n",
    "    print ('Average running time =',np.mean(runtime))\n",
    "    print ('Average RMSE =', np.mean(rmse))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
